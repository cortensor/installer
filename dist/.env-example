#===============================================================================
# Cortensor Node Configuration
#===============================================================================
# This is the configuration file for Cortensor Node
# Website: https://cortensor.network
# Discord: https://discord.gg/cortensor
# GitHub: https://github.com/cortensor
#
# IMPORTANT: This is an example configuration file.
# Make a copy of this file as '.env' and modify the values as needed.
#===============================================================================

#===============================================================================
# BLOCKCHAIN CONFIGURATION
#===============================================================================

# RPC Endpoints
#-------------------------------------------------------------------------------
# Arbitrum Sepolia RPC endpoint for testnet
# NOTE: HOST must always be set, regardless of multiple RPC host settings.
HOST=https://rpc.ankr.com/arbitrum_sepolia/f5f53d9133bbdf13a41ee88d81839470dc49d50dac0d7cf42e14f0045b98630c
CHAINID=421614

# Ethereum Mainnet RPC endpoint (for reference operations)
HOST_MAINNET=https://rpc.ankr.com/eth/0f4eb65057313c39f6fb51be08f77d42041a2ce92018518c5e02a8358029b4a0

# Multiple RPC Hosts (Optional)
#-------------------------------------------------------------------------------
# Set to 1 to enable multiple RPC host selection, 0 to disable
ENABLE_MULTIPLE_RPC_HOST=0
# Set to 1 to enable debug logging for multiple RPC host selection, 0 to disable
ENABLE_MULTIPLE_RPC_HOST_DEBUG=0
# Number of RPC hosts to use (1-20)
NUM_RPC_HOST=1
# RPC host list (set as many as NUM_RPC_HOST requires)
RPC_HOST_0=https://rpc.ankr.com/arbitrum_sepolia/f5f53d9133bbdf13a41ee88d81839470dc49d50dac0d7cf42e14f0045b98630c
RPC_HOST_1=
RPC_HOST_2=
RPC_HOST_3=
RPC_HOST_4=
RPC_HOST_5=
RPC_HOST_6=
RPC_HOST_7=
RPC_HOST_8=
RPC_HOST_9=
RPC_HOST_10=
RPC_HOST_11=
RPC_HOST_12=
RPC_HOST_13=
RPC_HOST_14=
RPC_HOST_15=
RPC_HOST_16=
RPC_HOST_17=
RPC_HOST_18=
RPC_HOST_19=

# Smart Contract Addresses
# DevNet 2
# CONTRACT_ADDRESS_RUNTIME="0xC0e4E569810a445d097CBB20e25775701f41A8cc"
# DevNet 3
# CONTRACT_ADDRESS_RUNTIME="0x5889b0E1620f133eFB93fab890543569DE846365"
# DevNet 4
# CONTRACT_ADDRESS_RUNTIME="0x7bDF2244a3Cc65335176d7e546Cc99B9316a912a"
# DevNet 5
# CONTRACT_ADDRESS_RUNTIME="0x8361E7821bDAD7F8F0aC7862Bebb190B8Da1A160"
# DevNet 6
# CONTRACT_ADDRESS_RUNTIME="0x2ACb5EE389B06250cC40593edbCc6eF3b9cEC8c7"\
# DevNet 7
# CONTRACT_ADDRESS_RUNTIME="0x8d67608D0F674F359DE0e420857739ECBDeb6B90"
# TestNet 1
# CONTRACT_ADDRESS_RUNTIME="0x0188C7F0c23c8be756F7b56486193E086f48E64b"
#-------------------------------------------------------------------------------
# TestNet 0 (Current)
CONTRACT_ADDRESS_RUNTIME="0xa438cE917a5740267e0f7217f81cbbAA23E7e106"

# Node Wallet Configuration
#-------------------------------------------------------------------------------
# IMPORTANT: Replace with your actual wallet address and private key
# NEVER share your private key or commit it to version control
NODE_PUBLIC_KEY=0x0000000000000000000000000000000000000000
NODE_PRIVATE_KEY=0x0000000000000000000000000000000000000000000000000000000000000000

#===============================================================================
# NETWORK CONFIGURATION
#===============================================================================

# WebSocket Configuration
#-------------------------------------------------------------------------------
# WebSocket - Used by Miner (router WebSocket endpoints for node communication)
WS_HOST_ROUTER="192.168.250.237"
WS_PORT_ROUTER="9001"

# WebSocket Public - Used by Router/REST server
# Router external IP and port for miner communication / public access
ROUTER_EXTERNAL_IP="192.168.250.221"
ROUTER_EXTERNAL_PORT="9001"

# Router REST Bind IP and Port for Client Communication
# Reverse proxy to this IP and port
ROUTER_REST_BIND_IP="127.0.0.1"
ROUTER_REST_BIND_PORT="5010"

# MCP HTTP Server for Router (Streamable MCP over HTTP)
#-------------------------------------------------------------------------------
# Set to 1 to enable the MCP HTTP server inside the router, 0 to disable
ROUTER_MCP=0

# Bind IP and port for the MCP HTTP server (used by MCP clients)
ROUTER_MCP_BIND_IP="127.0.0.1"
ROUTER_MCP_BIND_PORT="8001"

# Stream default session IDs (used when tool caller omits `session_id`)
#-------------------------------------------------------------------------------
# Default `session_id` for MCP completion tools
ROUTER_MCP_COMPLETION_SESSION_ID=""
# Default `session_id` for MCP delegate tools (`cortensor_delegate_v1`, `cortensor_delegate`)
ROUTER_MCP_DELEGATE_SESSION_ID=""
# Default `session_id` for MCP validate tools (`cortensor_validate_v1`, `cortensor_validate`)
ROUTER_MCP_VALIDATE_SESSION_ID=""
# Default `session_id` for MCP tasks tool (`cortensor_tasks`)
ROUTER_MCP_TASKS_SESSION_ID=""
# Global fallback `session_id` when a tool-specific key above is not set
ROUTER_MCP_DEFAULT_SESSION_ID=""

# MCP SSE (Legacy) Server for Router (HTTP + Server-Sent Events)
#-------------------------------------------------------------------------------
# Set to 1 to enable the legacy MCP SSE server inside the router, 0 to disable
ROUTER_MCP_SSE=0

# Bind IP and port for the MCP SSE server (used by SSE-capable MCP clients)
ROUTER_MCP_SSE_BIND_IP="127.0.0.1"
ROUTER_MCP_SSE_BIND_PORT="8000"

# SSE-specific session ID overrides (override stream defaults above)
#-------------------------------------------------------------------------------
# SSE-only override for completion tool default session
ROUTER_MCP_SSE_COMPLETION_SESSION_ID=""
# SSE-only override for delegate tool default session
ROUTER_MCP_SSE_DELEGATE_SESSION_ID=""
# SSE-only override for validate tool default session
ROUTER_MCP_SSE_VALIDATE_SESSION_ID=""
# SSE-only override for tasks tool default session
ROUTER_MCP_SSE_TASKS_SESSION_ID=""
# SSE-only global fallback session
ROUTER_MCP_SSE_DEFAULT_SESSION_ID=""

# MCP Session Policy + Routing Controls
#-------------------------------------------------------------------------------
# Force fixed sessions for all MCP tools.
# 1 = ignore caller-provided session_id and always use configured defaults.
ROUTER_MCP_ENFORCE_FIXED_SESSIONS=0

# Force x402 routing for non-trial MCP tools.
# 1 = MCP tools always use /api/*/x402/* (except trial tools).
ROUTER_MCP_FORCE_X402=0
# Stream-transport specific override (higher priority than ROUTER_MCP_FORCE_X402)
ROUTER_MCP_STREAM_FORCE_X402=0
# SSE-transport specific override (higher priority than ROUTER_MCP_FORCE_X402)
ROUTER_MCP_SSE_FORCE_X402=0

# MCP -> REST Timeout Settings (seconds)
#-------------------------------------------------------------------------------
# Shared fallback timeout for both stream and SSE MCP servers.
ROUTER_MCP_REST_TIMEOUT_SECONDS=300
# Streamable MCP-specific REST timeout override.
ROUTER_MCP_STREAM_REST_TIMEOUT_SECONDS=300
# SSE MCP-specific REST timeout override.
ROUTER_MCP_SSE_REST_TIMEOUT_SECONDS=300

# MCP Advertisement (GET /mcp/messages)
#-------------------------------------------------------------------------------
# Set to 1 to enable static MCP advertisement response on GET /mcp/messages.
MCP_ADVERTISE_ENABLED=0
# Optional JSON override merged into default advertisement payload.
MCP_ADVERTISE_JSON=""

#-------------------------------------------------------------------------------
# x402 Router Node Configuration
# These values control x402 payment settings exposed by the router REST API.
# They correspond to X402Config in the codebase.

# Set to 1 to enable x402 router node payment handling, 0 to disable
X402_ROUTER_NODE_ENABLE=0

# Network identifier for x402 payments (e.g. base-sepolia, base)
X402_ROUTER_NODE_NETWORK="base-sepolia"

# Address that will receive x402 payments (USDC on the configured network)
X402_ROUTER_NODE_PAY_TO=""

# Default price in USD for generic x402 endpoints (e.g. ping)
X402_ROUTER_NODE_PRICE_DEFAULT="0.001"

# Price in USD specifically for /api/v1/x402/completions/* endpoints
X402_ROUTER_NODE_PRICE_COMPLETIONS="0.001"

# x402 Trial Configuration (REST + MCP trial tools)
#-------------------------------------------------------------------------------
# Set to 1 to enable trial endpoints (/api/v1|v2/trial/*), 0 to disable.
X402_ROUTER_NODE_TRIAL_ENABLE=0

# Trial rate limit window: max requests allowed per client IP.
X402_ROUTER_NODE_TRIAL_MAX_REQUESTS=1
X402_ROUTER_NODE_TRIAL_WINDOW_SECONDS=900

# Optional dedicated trial session IDs.
# If empty, trial routes require session_id from caller.
X402_ROUTER_NODE_TRIAL_COMPLETION_SESSION_ID=""
X402_ROUTER_NODE_TRIAL_DELEGATE_SESSION_ID=""
X402_ROUTER_NODE_TRIAL_VALIDATE_SESSION_ID=""

#===============================================================================
# LLM ENGINE CONFIGURATION
#===============================================================================

# LLM Server Configuration
#-------------------------------------------------------------------------------
LLM_HOST="127.0.0.1"
LLM_PORT="8090"

# Memory Management
#-------------------------------------------------------------------------------
# Set to 1 to lock the model in memory for better performance
LLM_OPTION_MLOCK=1

# Context and Batch Configuration
#-------------------------------------------------------------------------------
# LLM / LLAMA Context Size
# Single Docker LLM Mode / 512
LLM_OPTION_CONTEXT_SIZE=512

# LLM / LLAMA Batch Size
# Single Docker LLM Mode / 512
LLM_OPTION_BATCH_SIZE=512

# GPU Configuration
#-------------------------------------------------------------------------------
# Set to 1 to enable GPU acceleration for the LLM
LLM_OPTION_GPU=0
# Threshold for GPU layers (-1 for auto)
# Single Docker LLM Mode
LLM_OPTION_GPU_THRESHOLD=-1

# Model-specific GPU thresholds (0-65) - Override general GPU threshold for specific models
# Only used when LLM_MEMORY_INDEX_DYNAMIC_LOADING=1 or worker manager is enabled
# -1 for auto, 0 to disable GPU for that model, positive number for specific threshold
# llava-v1.5-7b-q4
LLM_OPTION_MODEL_GPU_THRESHOLD_0=-1
# DeepSeek-R1-Distill-Llama-8B-Q4_K_M
LLM_OPTION_MODEL_GPU_THRESHOLD_1=-1
# Meta-Llama-3.1-8B-Instruct.Q4_K_M
LLM_OPTION_MODEL_GPU_THRESHOLD_2=-1
# Qwen_QwQ-32B-Q4_K_M
LLM_OPTION_MODEL_GPU_THRESHOLD_3=-1
# Mistral-7B-Instruct-v0.3.Q4_0
LLM_OPTION_MODEL_GPU_THRESHOLD_4=-1
# granite-3.2-8b-instruct-Q4_K_M
LLM_OPTION_MODEL_GPU_THRESHOLD_5=-1
# Qwen2.5-7B-Instruct-1M-Q4_K_M
LLM_OPTION_MODEL_GPU_THRESHOLD_6=-1
# google_gemma-3-4b-it-Q6_K
LLM_OPTION_MODEL_GPU_THRESHOLD_7=-1
# Qwen_Qwen3-4B-Q8_0
LLM_OPTION_MODEL_GPU_THRESHOLD_8=-1
# google_gemma-3-12b-it-Q4_K_M
LLM_OPTION_MODEL_GPU_THRESHOLD_9=-1
# DeepSeek-R1-Distill-Qwen-14B-Q4_K_M
LLM_OPTION_MODEL_GPU_THRESHOLD_10=-1
# Qwen2.5-Coder-14B-Instruct-Q4_K_M
LLM_OPTION_MODEL_GPU_THRESHOLD_11=-1
# Gemma3 270m
LLM_OPTION_MODEL_GPU_THRESHOLD_12=-1
# gpt-oss:20b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_13=-1
# deepseek-r1:8b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_14=-1
# granite4:3b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_15=-1
# gpt-oss:120b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_16=-1
# deepseek-r1:70b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_17=-1
# gemma3:4b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_18=-1
# gemma3:12b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_19=-1
# ministral-3:8b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_20=-1
# deepseek-r1:14b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_21=-1
# gemma3:27b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_22=-1
# qwen3-vl:4b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_23=-1
# qwen3-vl:8b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_24=-1
# ministral-3:14b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_25=-1
# qwen3:4b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_26=-1
# qwen3:8b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_27=-1
# mistral:7b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_28=-1
# phi3:3.8b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_29=-1
# phi3:14b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_30=-1
# llama3.2:1b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_31=-1
# llama3.2:3b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_32=-1
# llama3.1:8b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_33=-1
# llama3.1:70b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_34=-1
# phi4:14b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_35=-1
# dolphin3:8b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_36=-1
# tinyllama1.1b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_37=-1
# falcon3:3b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_38=-1
# falcon3:7b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_39=-1
# falcon3:10b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_40=-1
# devstral:24b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_41=-1
# qwen3-coder:30b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_42=-1
# llama4:16x17b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_43=-1
# llava:7b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_44=-1
# llava:13b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_45=-1
# llava:34b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_46=-1
# nemotron-3-nano:30b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_47=-1
# olmo-3:7b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_48=-1
# olmo-3:32b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_49=-1
# olmo2:7b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_50=-1
# olmo2:13b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_51=-1
# orca-mini:3b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_52=-1
# orca-mini:7b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_53=-1
# orca-mini:13b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_54=-1
# orca-mini:70b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_55=-1
# nomic-embed-text:v1.5-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_56=-1
# mxbai-embed-large:335m-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_57=-1
# bge-m3:567m-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_58=-1
# all-minilm:22m-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_59=-1
# all-minilm:33m-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_60=-1
# embeddinggemma:300m-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_61=-1
# snowflake-arctic-embed:335m-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_62=-1
# snowflake-arctic-embed2:568m-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_63=-1
# granite-embedding:278m-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_64=-1
# glm-4.7-flash:q4_K_M-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_65=-1

# CPU Configuration
#-------------------------------------------------------------------------------
# Number of CPU threads to use (-1 for auto)
# 99999 for maximum
LLM_OPTION_CPU_THREADS=-1

# Docker GPU Configuration
#-------------------------------------------------------------------------------
# Set to 1 to enable GPU support in Docker container
LLM_GPU_CONTAINER=0
# Specify GPU device IDs to use (e.g., "0,1" or leave empty for all available GPUs)
LLM_GPU_CONTAINER_DEVICE_IDS=""

#===============================================================================
# NODE OPERATION CONFIGURATION
#===============================================================================

# Logging
#-------------------------------------------------------------------------------
LOG_FILE_NAME="agent_miner.log"

# Debug/Development Mode
#-------------------------------------------------------------------------------
# Set to 1 to enable debug print statements
DEV_MODE_ENABLE_DEBUG_PRINT=1

# Environment Override
#-------------------------------------------------------------------------------
# Uncomment to override global environment file
# OVERRIDE_GLOBAL_ENV_FILE=path/to/env/file

# IPFS Configuration
#-------------------------------------------------------------------------------
# Set to 1 to enable IPFS in Docker
DOCKER_IPFS=0 # Not used

# Set to 1 to enable IPFS server, 0 to disable
ENABLE_IPFS_SERVER=1

# LLM Docker Configuration
#-------------------------------------------------------------------------------
# Set to 1 to use Docker for LLM
AGENT_MINER_DOCKER_LLM=1    # Not used
# Set to 1 to automatically start LLM Docker container
AGENT_MINER_DOCKER_LLM_START=1    # Not used
# Custom container image for LLM (only used with ENABLE_DEDICATED_NODE=1)
# Available models for dedicated nodes:
# 1 - DeepSeek-R1-Distill-Llama-8B-Q4_K_M
# 2 - Meta-Llama-3.1-8B-Instruct.Q4_K_M
# 3 - Qwen_QwQ-32B-Q4_K_M
# 4 - Mistral-7B-Instruct-v0.3.Q4_0
# 5 - granite-3.2-8b-instruct-Q4_K_M
# 6 - Qwen2.5-7B-Instruct-1M-Q4_K_M
# 7 - google_gemma-3-4b-it-Q6_K
# 8 - Qwen_Qwen3-4B-Q8_0
# 9 - google_gemma-3-12b-it-Q4_K_M
# 10 - DeepSeek-R1-Distill-Qwen-14B-Q4_K_M
# 11 - Qwen2.5-Coder-14B-Instruct-Q4_K_M
# 12 - gemma3:270m-gpu
# 13 - gpt-oss:20b-gpu
# 14 - deepseek-r1:8b-gpu
# 15 - granite4:3b-gpu
# 16 - gpt-oss:120b-gpu
# 17 - deepseek-r1:70b-gpu
# 18 - gemma3:4b-gpu
# 19 - gemma3:12b-gpu
# 20 - ministral-3:8b-gpu
# 21 - deepseek-r1:14b-gpu
# 22 - gemma3:27b-gpu
# 23 - qwen3-vl:4b-gpu
# 24 - qwen3-vl:8b-gpu
# 25 - ministral-3:14b-gpu
# 26 - qwen3:4b-gpu
# 27 - qwen3:8b-gpu
# 28 - mistral:7b-gpu
# 29 - phi3:3.8b-gpu
# 30 - phi3:14b-gpu
# 31 - llama3.2:1b-gpu
# 32 - llama3.2:3b-gpu
# 33 - llama3.1:8b-gpu
# 34 - llama3.1:70b-gpu
# 35 - phi4:14b-gpu
# 36 - dolphin3:8b-gpu
# 37 - tinyllama1.1b-gpu
# 38 - falcon3:3b-gpu
# 39 - falcon3:7b-gpu
# 40 - falcon3:10b-gpu
# 41 - devstral:24b-gpu
# 42 - qwen3-coder:30b-gpu
# 43 - llama4:16x17b-gpu
# 44 - llava:7b-gpu
# 45 - llava:13b-gpu
# 46 - llava:34b-gpu
# 47 - nemotron-3-nano:30b-gpu
# 48 - olmo-3:7b-gpu
# 49 - olmo-3:32b-gpu
# 50 - olmo2:7b-gpu
# 51 - olmo2:13b-gpu
# 52 - orca-mini:3b-gpu
# 53 - orca-mini:7b-gpu
# 54 - orca-mini:13b-gpu
# 55 - orca-mini:70b-gpu
# 56 - nomic-embed-text:v1.5-gpu
# 57 - mxbai-embed-large:335m-gpu
# 58 - bge-m3:567m-gpu
# 59 - all-minilm:22m-gpu
# 60 - all-minilm:33m-gpu
# 61 - embeddinggemma:300m-gpu
# 62 - snowflake-arctic-embed:335m-gpu
# 63 - snowflake-arctic-embed2:568m-gpu
# 64 - granite-embedding:278m-gpu
# 65 - glm-4.7-flash:q4_K_M-gpu
LLM_CONTAINER_IMAGE=""

# Subprocess model for LLM (only used when Docker is not used and ENABLE_DEDICATED_NODE=0)
# Available models:
# - KEY: Model Info
# 0 - default: llava-v1.5-7b-q4
#   - default-gpu: llava-v1.5-7b-q4
# 1 - DeepSeek-R1-Distill-Llama-8B-Q4_K_M: DeepSeek R1 Distill Llama 8B Q4_K_M
# 2 - Meta-Llama-3.1-8B-Instruct.Q4_K_M: Meta Llama 3.1 8B Instruct Q4_K_M
# 3 - Qwen_QwQ-32B-Q4_K_M: Qwen QwQ 32B Q4_K_M
# 4 - Mistral-7B-Instruct-v0.3.Q4_0: Mistral 7B Instruct v0.3 Q4_0
# 5 - granite-3.2-8b-instruct-Q4_K_M: Granite 3.2 8B Instruct Q4_K_M
# 6 - Qwen2.5-7B-Instruct-1M-Q4_K_M: Qwen 2.5 7B Instruct 1M Q4_K_M
# 7 - google_gemma-3-4b-it-Q6_K: Google Gemma 3 4B Q6_K
# 8 - Qwen_Qwen3-4B-Q8_0: Qwen Qwen3 4B Q8_0
# 9 - google_gemma-3-12b-it-Q4_K_M: Google Gemma 3 12B Q4_K_M
# 10 - DeepSeek-R1-Distill-Qwen-14B-Q4_K_M: DeepSeek R1 Distill Qwen 14B Q4_K_M
# 11 - Qwen2.5-Coder-14B-Instruct-Q4_K_M: Qwen 2.5 Coder 14B Instruct Q4_K_M
# 12 - gemma3:270m-gpu: Gemma 3 270M (Ollama GPU)
# 13 - gpt-oss:20b-gpu: GPT OSS 20B (Ollama GPU)
# 14 - deepseek-r1:8b-gpu: DeepSeek R1 8B (Ollama GPU)
# 15 - granite4:3b-gpu: Granite 4 3B (Ollama GPU)
# 16 - gpt-oss:120b-gpu: GPT OSS 120B (Ollama GPU)
# 17 - deepseek-r1:70b-gpu: DeepSeek R1 70B (Ollama GPU)
# 18 - gemma3:4b-gpu: Gemma 3 4B (Ollama GPU)
# 19 - gemma3:12b-gpu: Gemma 3 12B (Ollama GPU)
# 20 - ministral-3:8b-gpu: Ministral 3 8B (Ollama GPU)
# 21 - deepseek-r1:14b-gpu: DeepSeek R1 14B (Ollama GPU)
# 22 - gemma3:27b-gpu: Gemma 3 27B (Ollama GPU)
# 23 - qwen3-vl:4b-gpu: Qwen3-VL 4B (Ollama GPU)
# 24 - qwen3-vl:8b-gpu: Qwen3-VL 8B (Ollama GPU)
# 25 - ministral-3:14b-gpu: Ministral 3 14B (Ollama GPU)
# 26 - qwen3:4b-gpu: Qwen3 4B (Ollama GPU)
# 27 - qwen3:8b-gpu: Qwen3 8B (Ollama GPU)
# 28 - mistral:7b-gpu: Mistral 7B (Ollama GPU)
# 29 - phi3:3.8b-gpu: Phi-3 3.8B (Ollama GPU)
# 30 - phi3:14b-gpu: Phi-3 14B (Ollama GPU)
# 31 - llama3.2:1b-gpu: Llama 3.2 1B (Ollama GPU)
# 32 - llama3.2:3b-gpu: Llama 3.2 3B (Ollama GPU)
# 33 - llama3.1:8b-gpu: Llama 3.1 8B (Ollama GPU)
# 34 - llama3.1:70b-gpu: Llama 3.1 70B (Ollama GPU)
# 35 - phi4:14b-gpu: Phi-4 14B (Ollama GPU)
# 36 - dolphin3:8b-gpu: Dolphin 3 8B (Ollama GPU)
# 37 - tinyllama1.1b-gpu: TinyLlama 1.1B (Ollama GPU)
# 38 - falcon3:3b-gpu: Falcon 3 3B (Ollama GPU)
# 39 - falcon3:7b-gpu: Falcon 3 7B (Ollama GPU)
# 40 - falcon3:10b-gpu: Falcon 3 10B (Ollama GPU)
# 41 - devstral:24b-gpu: Devstral 24B (Ollama GPU)
# 42 - qwen3-coder:30b-gpu: Qwen3 Coder 30B (Ollama GPU)
# 43 - llama4:16x17b-gpu: Llama 4 16x17B (Ollama GPU)
# 44 - llava:7b-gpu: LLaVA 7B (Ollama GPU)
# 45 - llava:13b-gpu: LLaVA 13B (Ollama GPU)
# 46 - llava:34b-gpu: LLaVA 34B (Ollama GPU)
# 47 - nemotron-3-nano:30b-gpu: Nemotron-3 Nano 30B (Ollama GPU)
# 48 - olmo-3:7b-gpu: OLMo 3 7B (Ollama GPU)
# 49 - olmo-3:32b-gpu: OLMo 3 32B (Ollama GPU)
# 50 - olmo2:7b-gpu: OLMo 2 7B (Ollama GPU)
# 51 - olmo2:13b-gpu: OLMo 2 13B (Ollama GPU)
# 52 - orca-mini:3b-gpu: Orca Mini 3B (Ollama GPU)
# 53 - orca-mini:7b-gpu: Orca Mini 7B (Ollama GPU)
# 54 - orca-mini:13b-gpu: Orca Mini 13B (Ollama GPU)
# 55 - orca-mini:70b-gpu: Orca Mini 70B (Ollama GPU)
# 56 - nomic-embed-text:v1.5-gpu: Nomic Embed Text v1.5 (Ollama GPU)
# 57 - mxbai-embed-large:335m-gpu: MXBAI Embed Large 335M (Ollama GPU)
# 58 - bge-m3:567m-gpu: BGE M3 567M (Ollama GPU)
# 59 - all-minilm:22m-gpu: All MiniLM 22M (Ollama GPU)
# 60 - all-minilm:33m-gpu: All MiniLM 33M (Ollama GPU)
# 61 - embeddinggemma:300m-gpu: EmbeddingGemma 300M (Ollama GPU)
# 62 - snowflake-arctic-embed:335m-gpu: Snowflake Arctic Embed 335M (Ollama GPU)
# 63 - snowflake-arctic-embed2:568m-gpu: Snowflake Arctic Embed2 568M (Ollama GPU)
# 64 - granite-embedding:278m-gpu: Granite Embedding 278M (Ollama GPU)
# 65 - glm-4.7-flash:q4_K_M-gpu: GLM 4.7 Flash Q4_K_M (Ollama GPU)
LLM_SUBPROCESS_MODEL=""

# Dynamic Model Loading Configuration
#-------------------------------------------------------------------------------
# For Dynamic Model Loading, threshold of loading eligible models number in percentage
# Default is 100% which means if there are 10 eligible models, all 10 will be loaded
# 50% means if there are 10 eligible models, only 5 will be loaded
# 80% means if there are 10 eligible models, 8 will be loaded
LLM_MEMORY_INDEX_DYNAMIC_LOADING_THRESHOLD=100

# For Dynamic Model Loading, exclude model indexes
# Comma separated list of model indexes to exclude from dynamic loading
# Example: "6,9" excludes model indexes 6 and 9 from being loaded
LLM_MEMORY_INDEX_DYNAMIC_LOADING_EXCLUDE_MODEL_INDEXES=""

# Agent Role Configuration
#-------------------------------------------------------------------------------
# Role of this node in the network
AGENT_ROLE=minerv4    # Not used

# Dedicated Node Configuration
#-------------------------------------------------------------------------------
# Set to 0 to run as an ephemeral node, 1 for dedicated node
ENABLE_DEDICATED_NODE=0
# Comma-separated list of session IDs this node is authorized to serve
# Example: "0,1,2,3,4,5"
DEDICATED_NODE_AUTHORIZED_SESSIONS=""

# Queue Mode Configuration
#-------------------------------------------------------------------------------
# Set to 1 (default) to enable transaction queue mode for router nodes, 0 to disable
ENABLE_TX_QUEUE_MODE=1
# Set to 0 (default) to disable inference queue mode for miner nodes, 1 to enable
ENABLE_INFERENCE_QUEUE_MODE=0
# Set to 0 (default) to use blocking stream queue mode, 1 to enable non-blocking mode
ENABLE_STREAM_QUEUE_NON_BLOCKING=0

# Docker Configuration
#-------------------------------------------------------------------------------
# Set to 1 to enable Docker LLM Manager for handling multiple LLM containers, 0 to use single container mode
DOCKER_LLM_MANAGER=1

# LLM Worker Configuration
#-------------------------------------------------------------------------------
# Base port for LLM worker instances
LLM_WORKER_BASE_PORT=8090

# Port prefix for LLM worker instances (used for port assignment)
LLM_WORKER_PORT_PREFIX=0

# Container name prefix for LLM worker instances (for identification)
LLM_WORKER_CONTAINER_NAME_PREFIX=""

# LLM Memory Index Dynamic Loading
#-------------------------------------------------------------------------------
# Set to 1 to enable dynamic loading of memory indexes for LLM
LLM_MEMORY_INDEX_DYNAMIC_LOADING=1

# LLM Memory Index Buffer Percentage
# Percentage of additional memory buffer to add to model size calculations (default: 20%)
LLM_MEMORY_INDEX_BUFFER_PERCENTAGE=20

# LLM Gateway Configuration
#-------------------------------------------------------------------------------
# Set to 1 to enable LLM Gateway enforcement, 0 to disable
ENABLE_LLM_GATEWAY=0

# Base port for LLM Gateway worker instances
LLM_GATEWAY_WORKER_BASE_PORT=18888

# LLM startup timing configuration
#-------------------------------------------------------------------------------
# Wait time in seconds when starting LLM containers
LLM_START_WAIT_SECONDS=10

# Additional wait time in seconds for GPU-enabled containers
LLM_START_WAIT_SECONDS_ADDITIONAL_FOR_GPU=5

# Polling Intervals
#-------------------------------------------------------------------------------
# Interval in seconds for checking new network tasks
MINER_PULLING_INTERVAL_SECONDS=5

# L3 Configuration
#-------------------------------------------------------------------------------
# Set to 1 to run as an L3 node, 0 for L2
IS_L3=0

# Validator V2 Configuration
#-------------------------------------------------------------------------------
# Timeout in seconds for embedding batch calls in Validator V2
LLM_VALIDATOR_V2_EMBEDDING_BATCH_CALL_TIMEOUT=60
