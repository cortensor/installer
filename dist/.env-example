#===============================================================================
# Cortensor Node Configuration
#===============================================================================
# This is the configuration file for Cortensor Node
# Website: https://cortensor.network
# Discord: https://discord.gg/cortensor
# GitHub: https://github.com/cortensor
#
# IMPORTANT: This is an example configuration file.
# Make a copy of this file as '.env' and modify the values as needed.
#===============================================================================

#===============================================================================
# BLOCKCHAIN CONFIGURATION
#===============================================================================

# RPC Endpoints
#-------------------------------------------------------------------------------
# Arbitrum Sepolia RPC endpoint for testnet
HOST=https://rpc.ankr.com/arbitrum_sepolia/f5f53d9133bbdf13a41ee88d81839470dc49d50dac0d7cf42e14f0045b98630c
CHAINID=421614

# Ethereum Mainnet RPC endpoint (for reference operations)
HOST_MAINNET=https://rpc.ankr.com/eth/0f4eb65057313c39f6fb51be08f77d42041a2ce92018518c5e02a8358029b4a0

# Smart Contract Addresses
# DevNet 2
# CONTRACT_ADDRESS_RUNTIME="0xC0e4E569810a445d097CBB20e25775701f41A8cc"
# DevNet 3
# CONTRACT_ADDRESS_RUNTIME="0x5889b0E1620f133eFB93fab890543569DE846365"
# DevNet 4
# CONTRACT_ADDRESS_RUNTIME="0x7bDF2244a3Cc65335176d7e546Cc99B9316a912a"
# DevNet 5
# CONTRACT_ADDRESS_RUNTIME="0x8361E7821bDAD7F8F0aC7862Bebb190B8Da1A160"
# DevNet 6
# CONTRACT_ADDRESS_RUNTIME="0x2ACb5EE389B06250cC40593edbCc6eF3b9cEC8c7"\
# DevNet 7
# CONTRACT_ADDRESS_RUNTIME="0x8d67608D0F674F359DE0e420857739ECBDeb6B90"
# TestNet 1
# CONTRACT_ADDRESS_RUNTIME="0x0188C7F0c23c8be756F7b56486193E086f48E64b"
#-------------------------------------------------------------------------------
# TestNet 0 (Current)
CONTRACT_ADDRESS_RUNTIME="0xa438cE917a5740267e0f7217f81cbbAA23E7e106"
CONTRACT_ADDRESS_IAM="0x146834e5b45832769D8e8Fd0869AD1CEbdF150bD"
CONTRACT_ADDRESS_COGNITIVEV0="0x7DEe590Cb7B329e6454Ac78be4213d2D96FF195b"
CONTRACT_ADDRESS_NODESTATS="0x593EE6FBb0f7eFbaD8aE17d758878660158af788"
CONTRACT_ADDRESS_NODEREPUTATION="0xe62F741249C1eE21E053dac8aC3a7BD66Dc9c773"
CONTRACT_ADDRESS_NODEPOOL="0xFF489a46eeD2733dAa3bEa1e397E8b4ac97C1bb0"
CONTRACT_ADDRESS_ACL="0x6Dbc02BD4adbb34caeFb081fe60eDC41e393521B"

# Node Wallet Configuration
#-------------------------------------------------------------------------------
# IMPORTANT: Replace with your actual wallet address and private key
# NEVER share your private key or commit it to version control
NODE_PUBLIC_KEY=0x0000000000000000000000000000000000000000
NODE_PRIVATE_KEY=0x0000000000000000000000000000000000000000000000000000000000000000

#===============================================================================
# NETWORK CONFIGURATION
#===============================================================================

# WebSocket Configuration
#-------------------------------------------------------------------------------
# Router WebSocket endpoints for node communication
WS_HOST_ROUTER="192.168.250.237"
WS_PORT_ROUTER="9001"
AGENT_WS_HOST_ROUTER="192.168.250.237"
AGENT_WS_PORT_ROUTER="9001"

# Router External IP and Port for Miner Communication
# Used for external access to the router
ROUTER_EXTERNAL_IP="192.168.250.221"
ROUTER_EXTERNAL_PORT="9001"

# Router REST Bind IP and Port for Client Communication
# Reverse proxy to this IP and port
ROUTER_REST_BIND_IP="127.0.0.1"
ROUTER_REST_BIND_PORT="5010"

# MCP HTTP Server for Router (Streamable MCP over HTTP)
#-------------------------------------------------------------------------------
# Set to 1 to enable the MCP HTTP server inside the router, 0 to disable
ROUTER_MCP=0

# Bind IP and port for the MCP HTTP server (used by MCP clients)
ROUTER_MCP_BIND_IP="127.0.0.1"
ROUTER_MCP_BIND_PORT="8001"

#-------------------------------------------------------------------------------
# x402 Router Node Configuration
# These values control x402 payment settings exposed by the router REST API.
# They correspond to X402Config in the codebase.

# Set to 1 to enable x402 router node payment handling, 0 to disable
X402_ROUTER_NODE_ENABLE=0

# Network identifier for x402 payments (e.g. base-sepolia, base)
X402_ROUTER_NODE_NETWORK="base-sepolia"

# Address that will receive x402 payments (USDC on the configured network)
X402_ROUTER_NODE_PAY_TO=""

# Default price in USD for generic x402 endpoints (e.g. ping)
X402_ROUTER_NODE_PRICE_DEFAULT="0.001"

# Price in USD specifically for /api/v1/x402/completions/* endpoints
X402_ROUTER_NODE_PRICE_COMPLETIONS="0.001"

#===============================================================================
# LLM ENGINE CONFIGURATION
#===============================================================================

# LLM Server Configuration
#-------------------------------------------------------------------------------
LLM_HOST="127.0.0.1"
LLM_PORT="8090"

# Memory Management
#-------------------------------------------------------------------------------
# Set to 1 to lock the model in memory for better performance
LLM_OPTION_MLOCK=1

# Context and Batch Configuration
#-------------------------------------------------------------------------------
# LLM / LLAMA Context Size
# Single Docker LLM Mode / 512
LLM_OPTION_CONTEXT_SIZE=512

# LLM / LLAMA Batch Size
# Single Docker LLM Mode / 512
LLM_OPTION_BATCH_SIZE=512

# GPU Configuration
#-------------------------------------------------------------------------------
# Set to 1 to enable GPU acceleration for the LLM
LLM_OPTION_GPU=0
# Threshold for GPU layers (-1 for auto)
# Single Docker LLM Mode
LLM_OPTION_GPU_THRESHOLD=-1

# Model-specific GPU thresholds (0-40) - Override general GPU threshold for specific models
# Only used when LLM_MEMORY_INDEX_DYNAMIC_LOADING=1 or worker manager is enabled
# -1 for auto, 0 to disable GPU for that model, positive number for specific threshold
# llava-v1.5-7b-q4
LLM_OPTION_MODEL_GPU_THRESHOLD_0=-1
# DeepSeek-R1-Distill-Llama-8B-Q4_K_M
LLM_OPTION_MODEL_GPU_THRESHOLD_1=-1
# Meta-Llama-3.1-8B-Instruct.Q4_K_M
LLM_OPTION_MODEL_GPU_THRESHOLD_2=-1
# Qwen_QwQ-32B-Q4_K_M
LLM_OPTION_MODEL_GPU_THRESHOLD_3=-1
# Mistral-7B-Instruct-v0.3.Q4_0
LLM_OPTION_MODEL_GPU_THRESHOLD_4=-1
# granite-3.2-8b-instruct-Q4_K_M
LLM_OPTION_MODEL_GPU_THRESHOLD_5=-1
# Qwen2.5-7B-Instruct-1M-Q4_K_M
LLM_OPTION_MODEL_GPU_THRESHOLD_6=-1
# google_gemma-3-4b-it-Q6_K
LLM_OPTION_MODEL_GPU_THRESHOLD_7=-1
# Qwen_Qwen3-4B-Q8_0
LLM_OPTION_MODEL_GPU_THRESHOLD_8=-1
# google_gemma-3-12b-it-Q4_K_M
LLM_OPTION_MODEL_GPU_THRESHOLD_9=-1
# DeepSeek-R1-Distill-Qwen-14B-Q4_K_M
LLM_OPTION_MODEL_GPU_THRESHOLD_10=-1
# Qwen2.5-Coder-14B-Instruct-Q4_K_M
LLM_OPTION_MODEL_GPU_THRESHOLD_11=-1
# Gemma3 270m
LLM_OPTION_MODEL_GPU_THRESHOLD_12=-1
# gpt-oss:20b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_13=-1
# deepseek-r1:8b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_14=-1
# granite4:3b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_15=-1
# gpt-oss:120b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_16=-1
# deepseek-r1:70b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_17=-1
# gemma3:4b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_18=-1
# gemma3:12b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_19=-1
# ministral-3:8b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_20=-1
# deepseek-r1:14b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_21=-1
# gemma3:27b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_22=-1
# qwen3-vl:4b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_23=-1
# qwen3-vl:8b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_24=-1
# ministral-3:14b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_25=-1
# qwen3:4b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_26=-1
# qwen3:8b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_27=-1
# mistral:7b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_28=-1
# phi3:3.8b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_29=-1
# phi3:14b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_30=-1
# llama3.2:1b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_31=-1
# llama3.2:3b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_32=-1
# llama3.1:8b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_33=-1
# llama3.1:70b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_34=-1
# phi4:14b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_35=-1
# dolphin3:8b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_36=-1
# tinyllama1.1b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_37=-1
# falcon3:3b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_38=-1
# falcon3:7b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_39=-1
# falcon3:10b-gpu
LLM_OPTION_MODEL_GPU_THRESHOLD_40=-1

# CPU Configuration
#-------------------------------------------------------------------------------
# Number of CPU threads to use (-1 for auto)
# 99999 for maximum
LLM_OPTION_CPU_THREADS=-1

# Docker GPU Configuration
#-------------------------------------------------------------------------------
# Set to 1 to enable GPU support in Docker container
LLM_GPU_CONTAINER=0
# Specify GPU device IDs to use (e.g., "0,1" or leave empty for all available GPUs)
LLM_GPU_CONTAINER_DEVICE_IDS=""

#===============================================================================
# NODE OPERATION CONFIGURATION
#===============================================================================

# Logging
#-------------------------------------------------------------------------------
LOG_FILE_NAME="agent_miner.log"

# Debug/Development Mode
#-------------------------------------------------------------------------------
# Set to 1 to enable debug print statements
DEV_MODE_ENABLE_DEBUG_PRINT=1

# Environment Override
#-------------------------------------------------------------------------------
# Uncomment to override global environment file
# OVERRIDE_GLOBAL_ENV_FILE=path/to/env/file

# IPFS Configuration
#-------------------------------------------------------------------------------
# Set to 1 to enable IPFS in Docker
DOCKER_IPFS=0 # Not used

# Set to 1 to enable IPFS server, 0 to disable
ENABLE_IPFS_SERVER=1

# LLM Docker Configuration
#-------------------------------------------------------------------------------
# Set to 1 to use Docker for LLM
AGENT_MINER_DOCKER_LLM=1    # Not used
# Set to 1 to automatically start LLM Docker container
AGENT_MINER_DOCKER_LLM_START=1    # Not used
# Custom container image for LLM (only used with ENABLE_DEDICATED_NODE=1)
# Available models for dedicated nodes:
# 1 - DeepSeek-R1-Distill-Llama-8B-Q4_K_M
# 2 - Meta-Llama-3.1-8B-Instruct.Q4_K_M
# 3 - Qwen_QwQ-32B-Q4_K_M
# 4 - Mistral-7B-Instruct-v0.3.Q4_0
# 5 - granite-3.2-8b-instruct-Q4_K_M
# 6 - Qwen2.5-7B-Instruct-1M-Q4_K_M
# 7 - google_gemma-3-4b-it-Q6_K
# 8 - Qwen_Qwen3-4B-Q8_0
# 9 - google_gemma-3-12b-it-Q4_K_M
# 10 - DeepSeek-R1-Distill-Qwen-14B-Q4_K_M
# 11 - Qwen2.5-Coder-14B-Instruct-Q4_K_M
# 12 - gemma3:270m-gpu
# 13 - gpt-oss:20b-gpu
# 14 - deepseek-r1:8b-gpu
# 15 - granite4:3b-gpu
# 16 - gpt-oss:120b-gpu
# 17 - deepseek-r1:70b-gpu
# 18 - gemma3:4b-gpu
# 19 - gemma3:12b-gpu
# 20 - ministral-3:8b-gpu
# 21 - deepseek-r1:14b-gpu
# 22 - gemma3:27b-gpu
# 23 - qwen3-vl:4b-gpu
# 24 - qwen3-vl:8b-gpu
# 25 - ministral-3:14b-gpu
# 26 - qwen3:4b-gpu
# 27 - qwen3:8b-gpu
# 28 - mistral:7b-gpu
# 29 - phi3:3.8b-gpu
# 30 - phi3:14b-gpu
# 31 - llama3.2:1b-gpu
# 32 - llama3.2:3b-gpu
# 33 - llama3.1:8b-gpu
# 34 - llama3.1:70b-gpu
# 35 - phi4:14b-gpu
# 36 - dolphin3:8b-gpu
# 37 - tinyllama1.1b-gpu
# 38 - falcon3:3b-gpu
# 39 - falcon3:7b-gpu
# 40 - falcon3:10b-gpu
LLM_CONTAINER_IMAGE=""

# Subprocess model for LLM (only used when Docker is not used and ENABLE_DEDICATED_NODE=0)
# Available models:
# - KEY: Model Info
# 0 - default: llava-v1.5-7b-q4
#   - default-gpu: llava-v1.5-7b-q4
# 1 - DeepSeek-R1-Distill-Llama-8B-Q4_K_M: DeepSeek R1 Distill Llama 8B Q4_K_M
# 2 - Meta-Llama-3.1-8B-Instruct.Q4_K_M: Meta Llama 3.1 8B Instruct Q4_K_M
# 3 - Qwen_QwQ-32B-Q4_K_M: Qwen QwQ 32B Q4_K_M
# 4 - Mistral-7B-Instruct-v0.3.Q4_0: Mistral 7B Instruct v0.3 Q4_0
# 5 - granite-3.2-8b-instruct-Q4_K_M: Granite 3.2 8B Instruct Q4_K_M
# 6 - Qwen2.5-7B-Instruct-1M-Q4_K_M: Qwen 2.5 7B Instruct 1M Q4_K_M
# 7 - google_gemma-3-4b-it-Q6_K: Google Gemma 3 4B Q6_K
# 8 - Qwen_Qwen3-4B-Q8_0: Qwen Qwen3 4B Q8_0
# 9 - google_gemma-3-12b-it-Q4_K_M: Google Gemma 3 12B Q4_K_M
# 10 - DeepSeek-R1-Distill-Qwen-14B-Q4_K_M: DeepSeek R1 Distill Qwen 14B Q4_K_M
# 11 - Qwen2.5-Coder-14B-Instruct-Q4_K_M: Qwen 2.5 Coder 14B Instruct Q4_K_M
# 12 - gemma3:270m-gpu: Gemma 3 270M (Ollama GPU)
# 13 - gpt-oss:20b-gpu: GPT OSS 20B (Ollama GPU)
# 14 - deepseek-r1:8b-gpu: DeepSeek R1 8B (Ollama GPU)
# 15 - granite4:3b-gpu: Granite 4 3B (Ollama GPU)
# 16 - gpt-oss:120b-gpu: GPT OSS 120B (Ollama GPU)
# 17 - deepseek-r1:70b-gpu: DeepSeek R1 70B (Ollama GPU)
# 18 - gemma3:4b-gpu: Gemma 3 4B (Ollama GPU)
# 19 - gemma3:12b-gpu: Gemma 3 12B (Ollama GPU)
# 20 - ministral-3:8b-gpu: Ministral 3 8B (Ollama GPU)
# 21 - deepseek-r1:14b-gpu: DeepSeek R1 14B (Ollama GPU)
# 22 - gemma3:27b-gpu: Gemma 3 27B (Ollama GPU)
# 23 - qwen3-vl:4b-gpu: Qwen3-VL 4B (Ollama GPU)
# 24 - qwen3-vl:8b-gpu: Qwen3-VL 8B (Ollama GPU)
# 25 - ministral-3:14b-gpu: Ministral 3 14B (Ollama GPU)
# 26 - qwen3:4b-gpu: Qwen3 4B (Ollama GPU)
# 27 - qwen3:8b-gpu: Qwen3 8B (Ollama GPU)
# 28 - mistral:7b-gpu: Mistral 7B (Ollama GPU)
# 29 - phi3:3.8b-gpu: Phi-3 3.8B (Ollama GPU)
# 30 - phi3:14b-gpu: Phi-3 14B (Ollama GPU)
# 31 - llama3.2:1b-gpu: Llama 3.2 1B (Ollama GPU)
# 32 - llama3.2:3b-gpu: Llama 3.2 3B (Ollama GPU)
# 33 - llama3.1:8b-gpu: Llama 3.1 8B (Ollama GPU)
# 34 - llama3.1:70b-gpu: Llama 3.1 70B (Ollama GPU)
# 35 - phi4:14b-gpu: Phi-4 14B (Ollama GPU)
# 36 - dolphin3:8b-gpu: Dolphin 3 8B (Ollama GPU)
# 37 - tinyllama1.1b-gpu: TinyLlama 1.1B (Ollama GPU)
# 38 - falcon3:3b-gpu: Falcon 3 3B (Ollama GPU)
# 39 - falcon3:7b-gpu: Falcon 3 7B (Ollama GPU)
# 40 - falcon3:10b-gpu: Falcon 3 10B (Ollama GPU)
LLM_SUBPROCESS_MODEL=""

# Dynamic Model Loading Configuration
#-------------------------------------------------------------------------------
# For Dynamic Model Loading, threshold of loading eligible models number in percentage
# Default is 100% which means if there are 10 eligible models, all 10 will be loaded
# 50% means if there are 10 eligible models, only 5 will be loaded
# 80% means if there are 10 eligible models, 8 will be loaded
LLM_MEMORY_INDEX_DYNAMIC_LOADING_THRESHOLD=100

# For Dynamic Model Loading, exclude model indexes
# Comma separated list of model indexes to exclude from dynamic loading
# Example: "6,9" excludes model indexes 6 and 9 from being loaded
LLM_MEMORY_INDEX_DYNAMIC_LOADING_EXCLUDE_MODEL_INDEXES=""

# Agent Role Configuration
#-------------------------------------------------------------------------------
# Role of this node in the network
AGENT_ROLE=minerv1    # Not used

# Dedicated Node Configuration
#-------------------------------------------------------------------------------
# Set to 0 to run as an ephemeral node, 1 for dedicated node
ENABLE_DEDICATED_NODE=0
# Comma-separated list of session IDs this node is authorized to serve
# Example: "0,1,2,3,4,5"
DEDICATED_NODE_AUTHORIZED_SESSIONS=""

# Queue Mode Configuration
#-------------------------------------------------------------------------------
# Set to 1 (default) to enable transaction queue mode for router nodes, 0 to disable
ENABLE_TX_QUEUE_MODE=1
# Set to 0 (default) to disable inference queue mode for miner nodes, 1 to enable
ENABLE_INFERENCE_QUEUE_MODE=0
# Set to 0 (default) to use blocking stream queue mode, 1 to enable non-blocking mode
ENABLE_STREAM_QUEUE_NON_BLOCKING=0

# Docker Configuration
#-------------------------------------------------------------------------------
# Set to 1 to enable Docker LLM Manager for handling multiple LLM containers, 0 to use single container mode
DOCKER_LLM_MANAGER=1

# LLM Worker Configuration
#-------------------------------------------------------------------------------
# Base port for LLM worker instances
LLM_WORKER_BASE_PORT=8090

# Port prefix for LLM worker instances (used for port assignment)
LLM_WORKER_PORT_PREFIX=0

# Container name prefix for LLM worker instances (for identification)
LLM_WORKER_CONTAINER_NAME_PREFIX=""

# LLM Memory Index Dynamic Loading
#-------------------------------------------------------------------------------
# Set to 1 to enable dynamic loading of memory indexes for LLM
LLM_MEMORY_INDEX_DYNAMIC_LOADING=1

# LLM Memory Index Buffer Percentage
# Percentage of additional memory buffer to add to model size calculations (default: 20%)
LLM_MEMORY_INDEX_BUFFER_PERCENTAGE=20

# LLM Gateway Configuration
#-------------------------------------------------------------------------------
# Set to 1 to enable LLM Gateway enforcement, 0 to disable
ENABLE_LLM_GATEWAY=0

# Base port for LLM Gateway worker instances
LLM_GATEWAY_WORKER_BASE_PORT=18888

# LLM startup timing configuration
#-------------------------------------------------------------------------------
# Wait time in seconds when starting LLM containers
LLM_START_WAIT_SECONDS=10

# Additional wait time in seconds for GPU-enabled containers
LLM_START_WAIT_SECONDS_ADDITIONAL_FOR_GPU=5

# Polling Intervals
#-------------------------------------------------------------------------------
# Interval in seconds for checking new network tasks
MINER_PULLING_INTERVAL_SECONDS=5

# L3 Configuration
#-------------------------------------------------------------------------------
# Set to 1 to run as an L3 node, 0 for L2
IS_L3=0

# Validator V2 Configuration
#-------------------------------------------------------------------------------
# Timeout in seconds for embedding batch calls in Validator V2
LLM_VALIDATOR_V2_EMBEDDING_BATCH_CALL_TIMEOUT=60